{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> DS 3000 - Summer 2020</h2> </center>\n",
    "<center> <h3> DS Report </h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3> Predicting NBA Player Efficiency Rating </h3> </center>\n",
    "<center><h4>Nickhil Tekwani and Esha Aggarwal</h4></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executive Summary:\n",
    "\n",
    "Add your summary here (100-150 words)\n",
    "\n",
    "Provide a brief summary of your project. After reading this executive summary, your readers should have a rough understanding of what you did in this project. You can think of this summary in terms of the four sections of the report and write 1-2 sentences describing each section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. <a href='#1'>INTRODUCTION</a>\n",
    "2. <a href='#2'>METHOD</a>\n",
    "3. <a href='#3'>RESULTS</a>\n",
    "4. <a href='#4'>DISCUSSION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, orient your readers to your project. You've already written some of these in previous deliverables. Based on your final analysis, revise your problem statement and write a concise introduction section. This section should touch upon the following points, but should be written in full paragraphs. Your writing should incorporate all of these points (and more if you like) in a coherent way. Remember that you are trying to convince your readers that this is an important problem to tackle. \n",
    "\n",
    "Problem Statement\n",
    "* Describe the problem you would like to tackle. \n",
    "* What is the topic of your project? \n",
    "* What do you want to learn about it?\n",
    "\n",
    "Significance of the Problem\n",
    "* Why is it important to tackle this problem in your project?\n",
    "* In what ways could the insights from this project be useful?\n",
    "* **(optional)** Has there been previous work on your topic in terms of applying ML techniques to analyze similar datasets? Do some research into your topic. What algorithms were used? What was the performance of those algorithms? Cite your sources appropriately. You can use the numbered reference format or APA (if you are more comfortable with it).\n",
    "\n",
    "Questions/Hypothesis\n",
    "* End this section with a list of questions and hypotheses\n",
    "* You should tie these questions/hypotheses to the problem statement and its significance\n",
    "    * e.g. Given the aforementioned problem and its importance, we set out to tackle the following questions:\n",
    "    \n",
    "**Requirement:**\n",
    "* You should have at least one question tapping into the comparison of various machine learning algorithms in predicting your target variable from your features variables.\n",
    "* You should have at least one hypothesis regarding the relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Acquisition\n",
    "\n",
    "* Describe where you obtained your data. Provide a link to the original source. \n",
    "* If you scraped your data, include your code as a separate script file.\n",
    "* Your data should be stored in an online repository (e.g., GitHub) and your code should retrieve your data from that online resource. You can read csv files from the Web in the same way that you read files from local drive.\n",
    "* Describe the dataset (i.e., what it is about) and the number of variables included.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Variables\n",
    "* For your hypotheses, what are your IVs and DVs?\n",
    "* For your predictive models, what are your features and target variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Analysis\n",
    "* Specifically describe your predictive model. What outcome variable are you going to predict from what feature variables? Why do you think those are important predictors?\n",
    "* Describe why this is a supervised ML problem and identify the sub-category of the learning task (e.g. classification).\n",
    "* What machine learning algorithms are you going to use? **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Wrangling\n",
    "In this section, you should do the following and explain why you are doing what you are doing. For each, you should include your code in a cell, followed by a sample output. For instance, if you are one-hot encoding one of your variables, you should first describe what it is and why you are doing it. You should then include your code in a cell, and the sample output should be available as well.\n",
    "\n",
    "* Perform simple data cleaning (delete extra columns, deal with NA values, etc.)\n",
    "* Perform data wrangling to get your features and target values (e.g., grouping your dataframe by columns, applying functions to format dataframes, etc.)\n",
    "* Preprocess your variables (e.g., scaling/transforming feature variables to normalize them)\n",
    "* Perform feature extraction (dummy variables, new features from existing features, etc.)\n",
    "* Use one feature selection technique to select a subset of your original features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csvs and import pandas\n",
    "import pandas as pd\n",
    "player_data = pd.read_csv('data/player_data.csv')\n",
    "players = pd.read_csv('data/players.csv')\n",
    "season_stats = pd.read_csv('data/Seasons_Stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate names in PLAYER_DATA = \n",
      " name\n",
      "A.C. Green            1\n",
      "A.J. Bramlett         1\n",
      "A.J. English          1\n",
      "A.J. Guyton           1\n",
      "A.J. Hammons          1\n",
      "                     ..\n",
      "Zendon Hamilton       1\n",
      "Zhou Qi               1\n",
      "Zoran Dragic          1\n",
      "Zoran Planinic        1\n",
      "Zydrunas Ilgauskas    1\n",
      "Length: 4500, dtype: int64\n",
      "\n",
      " \n",
      " Duplicate names in PLAYERS = \n",
      " Player\n",
      "A.C. Green            1\n",
      "A.J. Bramlett         1\n",
      "A.J. English          1\n",
      "A.J. Guyton           1\n",
      "A.J. Hammons          1\n",
      "                     ..\n",
      "Zelmo Beaty*          1\n",
      "Zendon Hamilton       1\n",
      "Zoran Dragic          1\n",
      "Zoran Planinic        1\n",
      "Zydrunas Ilgauskas    1\n",
      "Length: 3921, dtype: int64\n",
      "\n",
      " \n",
      " Duplicate names in SEASON_STATS = \n",
      " Player\n",
      "A.C. Green            18\n",
      "A.J. Bramlett          1\n",
      "A.J. English           2\n",
      "A.J. Guyton            3\n",
      "A.J. Hammons           1\n",
      "                      ..\n",
      "Zelmo Beaty*           8\n",
      "Zendon Hamilton        8\n",
      "Zoran Dragic           3\n",
      "Zoran Planinic         3\n",
      "Zydrunas Ilgauskas    13\n",
      "Length: 3921, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking where duplicate player names exist (should only exist in player_data and players)\n",
    "pd_dupe = player_data.pivot_table(index=['name'], aggfunc='size')\n",
    "p_dupe = players.pivot_table(index=['Player'], aggfunc='size')\n",
    "ss_dupe = season_stats.pivot_table(index=['Player'], aggfunc='size')\n",
    "print(\"Duplicate names in PLAYER_DATA = \\n\", pd_dupe)\n",
    "print(\"\\n \\n Duplicate names in PLAYERS = \\n\", p_dupe)\n",
    "print(\"\\n \\n Duplicate names in SEASON_STATS = \\n\", ss_dupe)\n",
    "# based on this, it makes more sense to use player_data since it has more players \n",
    "# it also has more relevant information to what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "# function for converting height in \"feet-inches\" to inches\n",
    "def height_to_inches(h):\n",
    "    height = h.split('-')\n",
    "    feet = int(height[0])\n",
    "    inches = int(height[1])\n",
    "    total = (feet*12) + inches\n",
    "    return total\n",
    "# function for converting height in inches to cm\n",
    "def inches_to_cm(i):\n",
    "    return i * 2.54\n",
    "# function for converting weight in lbs to kgs\n",
    "def lbs_to_kgs(l):\n",
    "    return l * 0.453592\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING FUNCTION\n",
    "def clean_data():\n",
    "    # CLEAN SEASON STATS\n",
    "    # drop rows in season stats where PER = NaN\n",
    "    has_per_ss = season_stats[season_stats['PER'].notna()]\n",
    "    # drop columns from season stats that we dont need\n",
    "    useful_columns = ['Year', 'Player', 'Pos', 'Age', 'Tm', 'PER']\n",
    "    only_useful_ss = has_per_ss[useful_columns]\n",
    "    # strip asterisks\n",
    "    no_star = only_useful_ss.replace('\\*','',regex=True)\n",
    "    final_cleaned_ss = no_star\n",
    "\n",
    "    # CLEAN PLAYER DATA\n",
    "    # strip asterisks\n",
    "    no_player_star = player_data.replace('\\*','',regex=True)\n",
    "    # drop columns from player data that we dont need\n",
    "    useful_pd_col = [\"name\", \"year_start\", \"height\", \"weight\", \"college\"]\n",
    "    final_cleaned_pd = no_player_star[useful_pd_col]\n",
    "\n",
    "    # MERGE\n",
    "    merged_data = final_cleaned_pd.merge(final_cleaned_ss, left_on=['name', 'year_start'], right_on=[\"Player\", \"Year\"])\n",
    "    # drop duplicates (duplicate if name AND year_start are the same) but keep first instance (their rookie year)\n",
    "    merged_drop_dupes = merged_data.drop_duplicates([\"name\", \"year_start\"], keep=\"first\")\n",
    "    # drop Year and Player columns\n",
    "    dropYP = merged_drop_dupes.drop(labels=['Year', 'Player'], axis=1)\n",
    "    # lower case all column names\n",
    "    dropYP.columns = map(str.lower, dropYP.columns)\n",
    "    # change name of team column\n",
    "    dropYP[\"team\"] = dropYP[\"tm\"]\n",
    "    # drop old tm column\n",
    "    new_merged = dropYP.drop(labels=[\"tm\"], axis=1)\n",
    "    \n",
    "    # CONVERT HEIGHT TO CM\n",
    "    temp_1 = new_merged[\"height\"].apply(height_to_inches)\n",
    "    temp_2 = temp_1.apply(inches_to_cm)\n",
    "    new_merged[\"height\"] = temp_2\n",
    "    # CONVERT WEIGHT TO KGs\n",
    "    temp_3 = new_merged[\"weight\"].apply(lbs_to_kgs)\n",
    "    new_merged[\"weight\"] = temp_3\n",
    "    \n",
    "    # drop all NA and return df\n",
    "    result = new_merged.dropna(axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in variable\n",
    "cleaned_nba_df = clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling New Features\n",
    "* In the NBA, teams are grouped into the East and West Conference, so we think might provide more insight\n",
    "* than just looking at the 60 historical teams individually. Also, there are only 30 currently active teams,\n",
    "* so not all would be relevant to a future NBA scout, but conference will always be relevant even if new \n",
    "* teams are created or old ones are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of all the unqiue teams in dataframe\n",
    "teams = cleaned_nba_df[\"team\"].unique()\n",
    "# TOT means that the player played for multiple teams in their rookie year\n",
    "other = ('TOT')\n",
    "west_teams = ('POR', 'DEN', 'SAC', 'VAN', 'LAL', 'SDR', 'PHO','DAL', 'MEM', 'OKC', 'GSW', 'MIA', 'SAS', 'SFW', 'MIN',\n",
    "             'SEA', 'UTA', 'LAC', 'SDC', 'NOK', 'FTW', 'HOU', 'MNL', 'KCO', 'MLH', 'CHP', 'NOH', 'CHZ', 'NOP', 'INO')\n",
    "east_teams = ('MIL', 'SYR', 'DET', 'NYK', 'BOS', 'TOR', 'NJN', 'PHI', 'CLE', 'CHA', 'ORL', 'CIN', 'STL', 'CHI', 'PHW',\n",
    "             'BLB', 'ATL', 'WSB', 'WAS', 'CHH', 'IND', 'KCK', 'NOJ', 'BRK', 'ROC', 'BUF', 'CHO', 'CAP', 'BAL')\n",
    "\n",
    "# returns conference based on above lists\n",
    "def get_conference(c):\n",
    "    if(c in other):\n",
    "        return \"N/A\"\n",
    "    if(c in west_teams):\n",
    "        return \"West\"\n",
    "    if(c in east_teams):\n",
    "        return \"East\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA WRANGLING FUNCTION\n",
    "def wrangle_data():\n",
    "    team_col = cleaned_nba_df[\"team\"]\n",
    "    new_col = team_col.apply(get_conference)\n",
    "    cleaned_nba_df[\"conf\"] = new_col\n",
    "    return cleaned_nba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year_start</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>college</th>\n",
       "      <th>pos</th>\n",
       "      <th>age</th>\n",
       "      <th>per</th>\n",
       "      <th>team</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Alaa Abdelnaby</td>\n",
       "      <td>1991</td>\n",
       "      <td>208.28</td>\n",
       "      <td>108.862080</td>\n",
       "      <td>Duke University</td>\n",
       "      <td>PF</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>POR</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Zaid Abdul-Aziz</td>\n",
       "      <td>1969</td>\n",
       "      <td>205.74</td>\n",
       "      <td>106.594120</td>\n",
       "      <td>Iowa State University</td>\n",
       "      <td>C</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>TOT</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Kareem Abdul-Jabbar</td>\n",
       "      <td>1970</td>\n",
       "      <td>218.44</td>\n",
       "      <td>102.058200</td>\n",
       "      <td>University of California, Los Angeles</td>\n",
       "      <td>C</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>MIL</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Mahmoud Abdul-Rauf</td>\n",
       "      <td>1991</td>\n",
       "      <td>185.42</td>\n",
       "      <td>73.481904</td>\n",
       "      <td>Louisiana State University</td>\n",
       "      <td>PG</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>DEN</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Tariq Abdul-Wahad</td>\n",
       "      <td>1998</td>\n",
       "      <td>198.12</td>\n",
       "      <td>101.151016</td>\n",
       "      <td>San Jose State University</td>\n",
       "      <td>SG</td>\n",
       "      <td>23.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>SAC</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4059</td>\n",
       "      <td>George Zidek</td>\n",
       "      <td>1996</td>\n",
       "      <td>213.36</td>\n",
       "      <td>113.398000</td>\n",
       "      <td>University of California, Los Angeles</td>\n",
       "      <td>C</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CHH</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>Derrick Zimmerman</td>\n",
       "      <td>2006</td>\n",
       "      <td>190.50</td>\n",
       "      <td>88.450440</td>\n",
       "      <td>Mississippi State University</td>\n",
       "      <td>PG</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NJN</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4061</td>\n",
       "      <td>Stephen Zimmerman</td>\n",
       "      <td>2017</td>\n",
       "      <td>213.36</td>\n",
       "      <td>108.862080</td>\n",
       "      <td>University of Nevada, Las Vegas</td>\n",
       "      <td>C</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>ORL</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4063</td>\n",
       "      <td>Jim Zoet</td>\n",
       "      <td>1983</td>\n",
       "      <td>215.90</td>\n",
       "      <td>108.862080</td>\n",
       "      <td>Kent State University</td>\n",
       "      <td>C</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>DET</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4064</td>\n",
       "      <td>Bill Zopf</td>\n",
       "      <td>1971</td>\n",
       "      <td>185.42</td>\n",
       "      <td>77.110640</td>\n",
       "      <td>Duquesne University</td>\n",
       "      <td>PG</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>MIL</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3310 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  year_start  height      weight  \\\n",
       "0          Alaa Abdelnaby        1991  208.28  108.862080   \n",
       "1         Zaid Abdul-Aziz        1969  205.74  106.594120   \n",
       "4     Kareem Abdul-Jabbar        1970  218.44  102.058200   \n",
       "5      Mahmoud Abdul-Rauf        1991  185.42   73.481904   \n",
       "6       Tariq Abdul-Wahad        1998  198.12  101.151016   \n",
       "...                   ...         ...     ...         ...   \n",
       "4059         George Zidek        1996  213.36  113.398000   \n",
       "4060    Derrick Zimmerman        2006  190.50   88.450440   \n",
       "4061    Stephen Zimmerman        2017  213.36  108.862080   \n",
       "4063             Jim Zoet        1983  215.90  108.862080   \n",
       "4064            Bill Zopf        1971  185.42   77.110640   \n",
       "\n",
       "                                    college pos   age   per team  conf  \n",
       "0                           Duke University  PF  22.0  13.1  POR  West  \n",
       "1                     Iowa State University   C  22.0  12.3  TOT   N/A  \n",
       "4     University of California, Los Angeles   C  22.0  22.5  MIL  East  \n",
       "5                Louisiana State University  PG  21.0  12.2  DEN  West  \n",
       "6                 San Jose State University  SG  23.0  10.1  SAC  West  \n",
       "...                                     ...  ..   ...   ...  ...   ...  \n",
       "4059  University of California, Los Angeles   C  22.0   8.0  CHH  East  \n",
       "4060           Mississippi State University  PG  24.0   5.5  NJN  East  \n",
       "4061        University of Nevada, Las Vegas   C  20.0   7.3  ORL  East  \n",
       "4063                  Kent State University   C  29.0  -0.8  DET  East  \n",
       "4064                    Duquesne University  PG  22.0   9.6  MIL  East  \n",
       "\n",
       "[3310 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_df = wrangle_data()\n",
    "nba_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Exploration\n",
    "* Generate appropriate data visualizations for your key variables identified in the previous section\n",
    "* You should have at least three visualizations (and at least two different visualization types)\n",
    "* For each visualization provide an explanation regarding the variables involved and an interpretation of the graph.\n",
    "* If you are using Plotly, insert your visualizations as images as well (upload the graph images to an online source, e.g. github, and embed those into the cells in Jupyter Notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model Construction\n",
    "* Conduct your hypothesis test(s) here.\n",
    "* For your machine learning question(s), use the Training, Validation, and Testing approach through GridSearch\n",
    "* Apply machine learning algorithms (apply at least three different algorithms)\n",
    "* Train your algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Model Evaluation\n",
    "* Evaluate the performance of your algorithms on appropriate evaluation metrics, using your validation set\n",
    "    * Use at least two different metrics \n",
    "* Evaluate your results from multiple ML models and hypothesis tests\n",
    "    * What was the performance of each algorithm in plain English? Is there any indication of overfitting/underfitting?\n",
    "    * Was there a significant difference? Use the template from lecture slides when reporting the results of your hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Model Optimization\n",
    "* Tune your models using appropriate hyperparameters\n",
    "* Explain why you are doing this (e.g., to avoid overfitting, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Model Testing\n",
    "* Test your tuned algorithms using your testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DISCUSSION\n",
    "* Provide a summary of the steps you took to analyze your data and test your predictive model\n",
    "* Interpret your findings from 3.4., 3.5, and 3.6\n",
    "    * Which algorithms did you compare?\n",
    "    * Which algorithm(s) revealed best performance?\n",
    "    * Which algorithm(s) should be used for your predictive model?\n",
    "    * Based on your findings, can we you the features in your dataset to predict the outcome variable you identified using the algorithms you've applied? (It is okay if the answer is no. We're interested in the process, not the performance of the model.)\n",
    "* For your hypotheses, interpret the results. What does it mean to have significant/non-significant differences with regards to your data?\n",
    "\n",
    "\n",
    "* End this section with a conclusion paragraph containing some pointers for future work\n",
    "    * (e.g., get more data/features, perform another analysis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTRIBUTIONS\n",
    "* Describe each team member's contributions to the report (who did what in each section)\n",
    "* Remember this is a team effort!\n",
    "* Each member of your team will provide peer evaluation of other team members. Your final grade on the project will be based on those peer evaluations. An survey will be shared after the deadline for this deliverable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
